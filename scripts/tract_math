#!/usr/bin/env python
import sys
import itertools
from argparse import ArgumentParser, FileType, REMAINDER


def tract_math_operation(help_text, needs_one_tract=True):
    '''
    Decorator to identify tract_math functionalities the name of the
    function will be automatically incorporated to the tract_math options

    Parameters
    ----------
    help_text: help for the operation
    needs_one_tract: tells the script if all the input tractographies should
                      be unified as one or left as a tractography list
    '''
    def internal_decorator(func):
        func.help_text = help_text
        func.needs_one_tract = needs_one_tract
        return func
    return internal_decorator


def main():
    functions_list = dict((
        (f[0], f[1]) for f in globals().items() if hasattr(f[1], 'help_text')
    ))

    usage = r"""
    usage: %(prog)s <tract1.vtk> ... <tractN.vtk> operation <operation parameter1> ... <operation parameterN> <output_tract.vtk>

    Available operations:
    """

    function_list_names = functions_list.keys()
    function_list_names.sort()
    for f in function_list_names:
        usage += '\t%s %s\n' % (f, functions_list[f].help_text)

    #The first arguments, except for the last one, might be tractography files
    n_tracts = itertools.count(
        itertools.takewhile(
            lambda x: x not in function_list_names, sys.argv[1:-1]
        )
    )

    parser = ArgumentParser(usage=usage)
    parser.add_argument('tractographies', nargs=n_tracts, help='tractography files', type=FileType('r'))
    parser.add_argument('operation', type=str, choices=function_list_names,
                        help="operation to use")
    parser.add_argument('operation_parameters', type=str, nargs=REMAINDER,
                        help="operation parameters")

    args = parser.parse_args()
    #Load the global modules after the parsing of parameters
    global tr, vtkInterface, numpy, nibabel, tractography_from_vtk_files
    from tract_querier.tractography import tractography_from_vtk_files
    from tract_querier.tractography import tractography as tr
    from tract_querier.tractography import vtkInterface
    import numpy
    import nibabel

    if  (args.operation in functions_list) and functions_list[args.operation].needs_one_tract:
        tractography = tractography_from_vtk_files([f.name for f in args.tractographies])
    else:
        tractography = []
        try:
            for f in args.tractographies:
                tractography.append(tractography_from_vtk_files(f.name))
        except IOError as e:
            print >>sys.stderr, "Error reading file ", f.name, "(%s)" % repr(e)

    if  args.operation in functions_list:
        try:
            functions_list[args.operation](tractography, *args.operation_parameters)
        except TypeError:
            parser.error("Wrong number of parameters for the operation")
    else:
        parser.error("Operation not found")


@tract_math_operation(': counts the number of tracts')
def count(tractography):
    print len(tractography.tracts_to_process())


@tract_math_operation(': print the names of scalar data associated with each tract')
def scalars(tractography):
    for k in tractography.getOriginalData().keys():
        print k, ' '


@tract_math_operation(': calculates mean and std of fiber length')
def length_mean_std(tractography):
    lengths = numpy.empty(len(tractography.tracts_to_process()))

    for i, tract in enumerate(tractography.tracts_to_process()):
        lengths[i] = tract_length(tract)

    mean = lengths.mean()
    std = lengths.std()

    print mean, std
    return mean, std


def tract_length(tract):
    d2 = numpy.sqrt((numpy.diff(tract, axis=0) ** 2).sum(1))
    return d2.sum()


@tract_math_operation('<volume unit>: calculates the volume of a tract based on voxel occupancy of a certain voxel volume')
def tract_volume(tractography, resolution):
    resolution = float(resolution)
    voxels = voxelized_tract(tractography, resolution)

    neighbors = numpy.array([
        [0, 1, 0],
        [0, -1, 0],
        [1, 0, 0],
        [-1, 0, 0],
        [0, 0, 1],
        [0, 0, -1]
    ])
    dilated_voxels = set()
    dilated_voxels.update(voxels)
    eroded_voxels = set()
    for voxel in voxels:
        neighbors_list = zip(*(neighbors + voxel).T)
        dilated_voxels.update(neighbors_list)
        if len(voxels.intersection(neighbors_list)) == len(neighbors):
            eroded_voxels.add(voxel)

    #print len(dilated_voxels), len(voxels), len(eroded_voxels)
    approx_voxels = (len(dilated_voxels) - len(eroded_voxels)) / 2.
    print approx_voxels * (resolution ** 3)
    return approx_voxels * (resolution ** 3)


@tract_math_operation('<scalar>: calculates mean and std of a scalar quantity for each tract')
def scalar_tract_mean_std(tractography, scalar):
    try:
        tracts = tractography.original_tracts_data()[scalar]
        means = []
        stds = []
        for i, t in enumerate(tracts):
            means.append(t.mean().squeeze())
            stds.append(t.std().squeeze())
            print means[-1], stds[-1]

        return means, stds

    except KeyError:
        raise ValueError("Tractography does not contain this scalar data")


@tract_math_operation('<scalar>: calculates median of a scalar quantity for each tract')
def scalar_tract_median(tractography, scalar):
    try:
        tracts = tractography.original_tracts_data()[scalar]
        medians = []
        for i, t in enumerate(tracts):
            medians.append(numpy.median(t).squeeze())
            print medians[-1]

        return medians

    except KeyError:
        raise ValueError("Tractography does not contain this scalar data")


@tract_math_operation('<scalar>: calculates mean and std of a scalar quantity over tracts')
def scalar_mean_std(tractography, scalar):
    try:
        scalars = tractography.getOriginalData()[scalar]
        all_scalars = numpy.vstack(scalars)
        mean = all_scalars.mean(0)
        std = all_scalars.std(0)

        print mean.squeeze(), std.squeeze()
        return mean, std

    except KeyError:
        raise ValueError("Tractography does not contain this scalar data")


@tract_math_operation('<scalar>: calculates median of a scalar quantity over tracts')
def scalar_median(tractography, scalar):
    try:
        scalars = tractography.getOriginalData()[scalar]
        all_scalars = numpy.vstack(scalars)
        median = numpy.median(all_scalars)

        print median
        return median

    except KeyError:
        raise ValueError("Tractography does not contain this scalar data")


@tract_math_operation(': Minimum and maximum distance between two consecutive points')
def tract_point_distance_min_max(tractography):
    dist_min = numpy.empty(len(tractography.tracts_to_process()))
    dist_max = numpy.empty(len(tractography.tracts_to_process()))
    for i, tract in enumerate(tractography.tracts_to_process()):
        dist = tract_length(tract)
        dist_min[i] = dist.min()
        dist_max[i] = dist.max()
    print dist_min.min(), dist_max.max()


@tract_math_operation('<points per tract> <tractography_file_output>: subsamples tracts to a maximum number of points')
def tract_subsample(tractography, points_per_tract, tractography_file_output):
    tractography.subsample_tracts(int(points_per_tract))
    vtkInterface.writeLinesToVtkPolyData(
        tractography_file_output, tractography.tracts_to_process(),  tractography.tracts_data_to_process()
    )


@tract_math_operation('<mm per tract> <tractography_file_output>: subsamples tracts to a maximum number of points')
def tract_remove_short_fibers(tractography, min_tract_length, tractography_file_output):

    min_tract_length = float(min_tract_length)

    tracts = tractography.tracts_to_process()
    data = tractography.tracts_data_to_process()

    tract_ix_to_keep = [
        i for i, tract in enumerate(tractography.tracts_to_process())
        if tract_length(tract) > min_tract_length
    ]

    selected_tracts = [tracts[i] for i in tract_ix_to_keep]

    selected_data = dict()
    for key, item in data.items():
        if len(item) == len(tracts):
            selected_data_items = [item[i] for i in tract_ix_to_keep]
            selected_data[key] = selected_data_items
        else:
            selected_data[key] = item

    vtkInterface.writeLinesToVtkPolyData(
        tractography_file_output, selected_tracts, selected_data)


@tract_math_operation('<thickness> <subsampling> <tractography1.vtk> ... <tractographyN.vtk>: measures the volume overlap of one tract with others')
def tract_tensor_similarity(tractography, thickness, subsampling, *tracts):
    from tract_querier.tensor_covariance import fiberSegments
    thickness = float(thickness)
    subsampling = int(subsampling)

    tractography.subsample_tracts(subsampling)
    t_gp = [fiberSegments.FiberSegmentGaussianProcess(t, thickness) for t in tractography.tracts_to_process() if len(t) > 2]
    bundle = fiberSegments.FiberBundleSegmentGaussianProcess(t_gp)
    bundle_norm = bundle * bundle
    for tract in tracts:
        tract = tractography_from_vtk_files(tract)
        tract.subsample_tracts(subsampling)
        t_gp1 = [fiberSegments.FiberSegmentGaussianProcess(t, thickness) for t in tract.tracts_to_process() if len(t) > 2]
        bundle1 = fiberSegments.FiberBundleSegmentGaussianProcess(t_gp1)
        print (bundle * bundle1) / numpy.sqrt(bundle_norm * (bundle1 * bundle1))


@tract_math_operation('<image> <quantity_name> <tractography_file_output>: maps the values of an image to the tract points')
def tract_map_image(tractography, image, quantity_name, tractography_file_output):
    from os import path
    from scipy import ndimage

    image = nibabel.load(image)

    ijk_points = tract_in_ijk(image, tractography)
    image_data = image.get_data()

    if image_data.ndim > 3:
        output_name, ext = path.splitext(tractography_file_output)
        output_name = output_name + '_%04d' + ext
        for i, image in enumerate(image_data):
            new_scalar_data = ndimage.map_coordinates(
                image.T, ijk_points.T
            )[:, None]
            tractography.original_tracts_data()[quantity_name] = new_scalar_data
            vtkInterface.writeLinesToVtkPolyData(output_name % i, tractography.original_tracts(),  tractography.original_tracts_data())
    else:
        new_scalar_data = ndimage.map_coordinates(
            image_data.T, ijk_points.T
        )[:, None]
        tractography.original_tracts_data()[quantity_name] = new_scalar_data
        vtkInterface.writeLinesToVtkPolyData(
            tractography_file_output, tractography.original_tracts(),  tractography.original_tracts_data()
        )


@tract_math_operation('<image> <mask_out>: calculates the mask image from a tract on the space of the given image')
def tract_generate_mask(tractography, image, image_out):
    image = nibabel.load(image)

    mask = tract_mask(image, tractography)

    nibabel.save(nibabel.spatialimages.SpatialImage(mask, image.get_affine()), image_out)


@tract_math_operation('<image> [smoothing] <image_out>: calculates the probabilistic tract image for these tracts', needs_one_tract=False)
def tract_generate_population_probability_map(tractographies, image, *args):
    from scipy import ndimage
    image = nibabel.load(image)

    if len(args) > 1:
        smoothing = float(args[0])
        image_out = args[1]
    else:
        smoothing = 0
        image_out = args[0]

    if isinstance(tractographies, tr.Tractography):
        tractographies = [tractographies]

    prob_map = tract_mask(image, tractographies[0]).astype(float)
    if smoothing > 0:
	prob_map = ndimage.gaussian_filter(prob_map, smoothing)

    for tract in tractographies[1:]:
        aux_map = tract_mask(image, tract)
	if smoothing > 0:
		aux_map = ndimage.gaussian_filter(aux_map, smoothing)
	prob_map += aux_map

    prob_map /= len(tractographies)

    nibabel.save(
        nibabel.spatialimages.SpatialImage(prob_map, image.get_affine()),
        image_out
    )


@tract_math_operation('<image> <image_out>: calculates the probabilistic tract image for these tracts', needs_one_tract=False)
def tract_generate_probability_map(tractographies, image, image_out):
    image = nibabel.load(image)

    prob_map = tract_probability_map(image, tractographies[0]).astype(float)

    for tract in tractographies[1:]:
        new_prob_map = tract_mask(image, tract)
        prob_map = prob_map + new_prob_map - (prob_map * new_prob_map)

    nibabel.save(nibabel.spatialimages.SpatialImage(prob_map, image.get_affine()), image_out)


@tract_math_operation('<tractography_out>: takes the union of all tractographies', needs_one_tract=False)
def tract_merge(tractographies, tractography_file_output):
    all_tracts = []
    all_data = {}
    keys = [set(t.tracts_data_to_process().keys()) for t in tractographies]
    common_keys = keys[0].intersection(*keys[1:])
    for tract in tractographies:
        tracts = tract.tracts_to_process()
        all_tracts += tract.tracts_to_process()
        data = tract.tracts_data_to_process()
        for k in common_keys:
            if len(data[k]) == len(tracts):
                if k not in all_data:
                    all_data[k] = []
                all_data[k] += data[k]
            else:
                all_data[k] = data[k]

    vtkInterface.writeLinesToVtkPolyData(
        tractography_file_output,
        all_tracts, all_data
    )


@tract_math_operation('<volume unit> <tract1.vtk> ... <tractN.vtk>: calculates the kappa value of the first tract with the rest in the space of the reference image')
def tract_kappa(tractography, resolution, *other_tracts):
    resolution = float(resolution)

    voxels = voxelized_tract(tractography, resolution)

    for tract in other_tracts:
        voxels1 = voxelized_tract(tractography_from_vtk_files(tract), resolution)

        all_voxels = numpy.array(list(voxels.union(voxels1)))
        N = (all_voxels.max(0) - all_voxels.min(0)).prod()
        pp = len(voxels.intersection(voxels1)) * 1.
        pn = len(voxels.difference(voxels1)) * 1.
        np = len(voxels1.difference(voxels)) * 1.
        nn = N - pp - pn - np
        observed_agreement = (pp + nn) / N
        chance_agreement = ((pp + pn) * (pp + np) + (nn + np) * (nn + pn)) / (N * N)

        k = (observed_agreement - chance_agreement) / (1 - chance_agreement)

        print k


@tract_math_operation('<volume> <threshold> <tract1.vtk> ... <tractN.vtk>: calculates the kappa value of the first tract with the rest in the space of the reference image')
def tract_kappa_volume(tractography, volume, threshold, *other_tracts):
    resolution = float(resolution)

    volume = nibabel.load(volume)
    mask = (volume.get_data() > threshold).astype(int)
    voxels = tract_mask(mask, tractography)

    for tract in other_tracts:
        voxels1 = voxelized_tract(tractography_from_vtk_files(tract), resolution)

        all_voxels = numpy.array(list(voxels.union(voxels1)))
        N = (all_voxels.max(0) - all_voxels.min(0)).prod()
        pp = len(voxels.intersection(voxels1)) * 1.
        pn = len(voxels.difference(voxels1)) * 1.
        np = len(voxels1.difference(voxels)) * 1.
        nn = N - pp - pn - np
        observed_agreement = (pp + nn) / N
        chance_agreement = ((pp + pn) * (pp + np) + (nn + np) * (nn + pn)) / (N * N)

        k = (observed_agreement - chance_agreement) / (1 - chance_agreement)

        print k


@tract_math_operation('<volume unit> <tract1.vtk> ... <tractN.vtk>: calculates the dice coefficient of the first tract with the rest in the space of the reference image')
def tract_dice(tractography, resolution, *other_tracts):
    resolution = float(resolution)

    voxels = voxelized_tract(tractography, resolution)

    for tract in other_tracts:
        voxels1 = voxelized_tract(tractography_from_vtk_files(tract), resolution)
        print 2 * len(voxels.intersection(voxels1)) * 1. / (len(voxels) + len(voxels1))


def voxelized_tract(tractography, resolution):
    from itertools import izip
    all_points = numpy.vstack(tractography.tracts_to_process())
    all_points /= resolution
    all_points = all_points.round(0).astype(int)
    return set(izip(*(all_points.T)))


@tract_math_operation('<var> <tract_out>: smoothes the tract by convolving with a sliding window')
def tract_smooth(tractography, var, tractography_file_output):
    from sklearn.neighbors import BallTree

    var = float(var)
    std = var ** 2

    points = tractography.original_tracts()

    all_points = numpy.vstack(points)
    bt = BallTree(all_points)
    N = len(all_points) / 3
    I = numpy.eye(3)[None, ...]
    for i, tract in enumerate(tractography.original_tracts()):
        #all_points = numpy.vstack(points[:i] + points[i + 1:])
        #bt = BallTree(all_points)

        diff = numpy.diff(tract, axis=0)
        diff = numpy.vstack((diff, diff[-1]))
        lengths = numpy.sqrt((diff ** 2).sum(1))
        #cum_lengths = numpy.cumsum(lengths)

        diff_norm = diff / lengths[:, None]
        tangent_lines = diff_norm[:, None, :] * diff_norm[:, :, None]
        normal_planes = I - tangent_lines
#        weight_matrices = normal_planes + 1e10 * tangent_lines

        N = max(len(d) for d in bt.query_radius(tract, var * 3))

        close_point_distances, close_point_indices = bt.query(
            tract, N
        )

        close_points = all_points[close_point_indices]
        difference_vectors = close_points - tract[:, None, :]
        projected_vectors = (
            normal_planes[:, None, :] *
            difference_vectors[..., None]
        ).sum(-2)
        projected_points = projected_vectors + tract[:, None, :]
        #projected_distances2 = (projected_vectors**2).sum(-1)
        #projected_weights = numpy.exp(- .5 * projected_distances2 / std)
        #projected_weights /= projected_weights.sum(-1)[:, None]

        weights = numpy.exp(
            -.5 * close_point_distances ** 2 / std
        )[..., None]
        weights /= weights.sum(-2)[..., None]

        #tract += (weights * projected_vectors).sum(-2)

#        weighted_distances = (
#            weight_matrices[:, None, :] *
#            difference_vectors[..., None]
#        ).sum(-2)
#        weighted_distances *= difference_vectors
#        weighted_distances = weighted_distances.sum(-1) ** .5
        #weighted_points = (projected_points * weights).sum(1)

        weighted_points = (projected_points * weights).sum(1)

        #import ipdb
        #ipdb.set_trace()
        tract[:] = weighted_points
        #tract /= norm_term

    vtkInterface.writeLinesToVtkPolyData(
        tractography_file_output,
        tractography.original_tracts(),  tractography.original_tracts_data()
    )


def tract_mask(image, tractography):
    ijk_points = tract_in_ijk(image, tractography)
    image_data = image.get_data()

    ijk_clipped = ijk_points.clip(
        (0, 0, 0), numpy.array(image_data.shape) - 1
    ).astype(int)

    mask = numpy.zeros_like(image_data, dtype=float)
    mask[tuple(ijk_clipped.T)] = 1
    return mask


def tract_probability_map(image, tractography):
    ijk_tracts = each_tract_in_ijk(image, tractography)
    image_data = image.get_data()

    probability_map = numpy.zeros_like(image_data, dtype=float)
    for ijk_points in ijk_tracts:
        ijk_clipped = ijk_points.clip(
            (0, 0, 0), numpy.array(image_data.shape) - 1
        ).astype(int)

        probability_map[tuple(ijk_clipped.T)] += 1

    probability_map /= len(ijk_tracts)
    return probability_map


def each_tract_in_ijk(image, tractography):
    ijk_tracts = []
    for tract in tractography.tracts_to_process():
        ijk_tracts.append(numpy.dot(numpy.linalg.inv(image.get_affine()), numpy.hstack((
            tract,
            numpy.ones((len(tract), 1))
        )).T).T[:, :-1])
    return ijk_tracts


def tract_in_ijk(image, tractography):
    ras_points = numpy.vstack(tractography.tracts_to_process())
    ijk_points = numpy.dot(numpy.linalg.inv(image.get_affine()), numpy.hstack((
        ras_points,
        numpy.ones((len(ras_points), 1))
    )).T).T[:, :-1]
    return ijk_points


if __name__ == "__main__":
    main()
    sys.exit()
